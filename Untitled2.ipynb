{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPSpfva8EbbgNN6Xfb/XuWa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aYUSHpAAL/Project/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIUTMJLPf13R",
        "outputId": "6c335eaf-93c0-45f2-8eed-c9ea0b3b6c54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m114.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "pip install tiktoken matplotlib torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "import matplotlib\n",
        "import tiktoken\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 1024, # Context length : represents the model's maximum input token count\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "batch =torch.randn(2, 5)\n",
        "\n",
        "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
        "out = layer(batch)\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.shape) == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "batch_size, d_in = batch.shape # Only unpack 2 values\n",
        "context_length = batch.shape[1] # Assign context_length separately\n",
        "# batch_size, context_length, d_in = batch.shape # Original problematic line\n",
        "d_out = 2 # Output dimension\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "out = mha(batch)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        # Use a placeholder for TransformerBlock\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        # Use a placeholder for LayerNorm\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        x = (x - mean) / (std + self.eps)\n",
        "        x = self.scale * x + self.shift\n",
        "        return x\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval()\n",
        "class GPTDataset(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "def create_dataloader(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDataset(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (batch, n_tokens) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest probability value\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "x = torch.rand(2, 4, 768)  # Shape: [batch_size, num_tokens, emb_dim]\n",
        "block = TransformerBlock(GPT_CONFIG_124M)\n",
        "out = block(x)\n",
        "\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "file_path = \"both book.txt\"\n",
        "url = \"https://github.com/aYUSHpAAL/Project/blob/bd632fc071ec843ffc72f3bd9fa48ccf908c5c06/both%20book.txt\"\n",
        "if not os.path.exists(file_path):\n",
        "    urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"Now, Kitty, you may cough as much as you choose,\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text))\n",
        "train_data = text[:split_idx]\n",
        "val_data = text[split_idx:]\n",
        "train_loader = create_dataloader(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride= GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
        "\n",
        "\n",
        "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
        "\n",
        "\n",
        "\n",
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n",
        "\n",
        "\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "\n",
        "\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 5\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
        "    start_context=\"Now, Kitty, you may cough as much as you choose,\", tokenizer=tokenizer\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 914
        },
        "id": "BGJtzXBdiE6Y",
        "outputId": "804e09b5-64d1-45fb-fa9e-9ce42f24c456"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 9.202, Val loss 9.187\n",
            "Ep 1 (Step 000050): Train loss 5.969, Val loss 5.332\n",
            "Ep 1 (Step 000100): Train loss 5.065, Val loss 5.012\n",
            "Ep 1 (Step 000150): Train loss 4.436, Val loss 4.882\n",
            "Ep 1 (Step 000200): Train loss 4.545, Val loss 4.724\n",
            "Ep 1 (Step 000250): Train loss 4.503, Val loss 4.616\n",
            "Now, Kitty, you may cough as much as you choose, and\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\\n",
            "Ep 2 (Step 000300): Train loss 4.405, Val loss 4.687\n",
            "Ep 2 (Step 000350): Train loss 4.499, Val loss 4.569\n",
            "Ep 2 (Step 000400): Train loss 4.296, Val loss 4.518\n",
            "Ep 2 (Step 000450): Train loss 4.152, Val loss 4.494\n",
            "Ep 2 (Step 000500): Train loss 4.165, Val loss 4.462\n",
            "Ep 2 (Step 000550): Train loss 3.787, Val loss 4.451\n",
            "Now, Kitty, you may cough as much as you choose, and\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\\n",
            "Ep 3 (Step 000600): Train loss 3.842, Val loss 4.408\n",
            "Ep 3 (Step 000650): Train loss 3.942, Val loss 4.403\n",
            "Ep 3 (Step 000700): Train loss 3.633, Val loss 4.409\n",
            "Ep 3 (Step 000750): Train loss 3.518, Val loss 4.335\n",
            "Ep 3 (Step 000800): Train loss 3.760, Val loss 4.352\n",
            "Ep 3 (Step 000850): Train loss 3.549, Val loss 4.334\n",
            "Now, Kitty, you may cough as much as you choose, and\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\\n",
            "Ep 4 (Step 000900): Train loss 3.434, Val loss 4.324\n",
            "Ep 4 (Step 000950): Train loss 3.654, Val loss 4.321\n",
            "Ep 4 (Step 001000): Train loss 3.511, Val loss 4.304\n",
            "Ep 4 (Step 001050): Train loss 3.623, Val loss 4.326\n",
            "Ep 4 (Step 001100): Train loss 3.680, Val loss 4.293\n",
            "Ep 4 (Step 001150): Train loss 3.561, Val loss 4.307\n",
            "Now, Kitty, you may cough as much as you choose, and\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\\n",
            "Ep 5 (Step 001200): Train loss 3.461, Val loss 4.296\n",
            "Ep 5 (Step 001250): Train loss 3.220, Val loss 4.311\n",
            "Ep 5 (Step 001300): Train loss 3.220, Val loss 4.328\n",
            "Ep 5 (Step 001350): Train loss 2.909, Val loss 4.304\n",
            "Ep 5 (Step 001400): Train loss 2.849, Val loss 4.271\n",
            "Ep 5 (Step 001450): Train loss 3.103, Val loss 4.289\n",
            "Now, Kitty, you may cough as much as you choose, and\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\r\",\"\\\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}